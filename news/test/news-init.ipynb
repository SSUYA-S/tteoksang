{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 프로그램 시작시 해 줘야 하는 부분\n",
    "# 터미널은 conda activate venv 하고 시작\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICES_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"8\"\n",
    "\n",
    "\n",
    "# 크롤러를 만들기 전 필요한 도구 임포트\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import os\n",
    "import urllib3\n",
    "import asyncio\n",
    "import requests\n",
    "import aiohttp\n",
    "import traceback\n",
    "\n",
    "# 시작 날짜와 끝 날짜, 현재 하고 있는 날짜\n",
    "# start_date = '20220808'\n",
    "# end_date = '20220813'\n",
    "# doing_date = start_date\n",
    "news_code = 101 # 뉴스 코드\n",
    "start = 1 # url 크롤링 시작 페이지(default = 1)\n",
    "end = 500 # url 크롤링 끝 페이지\n",
    "stop_count = 150\n",
    "stop_page = 25\n",
    "\n",
    "# 뉴스 코드 리스트\n",
    "idx2word = {'101' : '경제', '102' : '사회', '103' : '생활/문화', '105' : 'IT/과학'}\n",
    "\n",
    "# 페이지 수, 카테고리, 날짜를 입력값으로 받기\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.90 Safari/537.36'}\n",
    "\n",
    "# url 리스트받아오기\n",
    "async def make_urllist(page_start, page_end, code, date, news_data, doing_date, session, day_count):\n",
    "  print('뉴스 기사 하루 시작  '+ doing_date)\n",
    "  page_count = 0\n",
    "  url_list = []\n",
    "  first_url=''\n",
    "  for i in range(page_start, page_end + 1):\n",
    "    url = 'https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1='+str(code)+'&date='+str(date)+'&page='+str(i)\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.90 Safari/537.36'}\n",
    "\n",
    "    # ssl 인증서에 대한 오류(신뢰할 수 없는 ssl 인증서)가 생기기 때문에 verify=False 를 추가해준다.\n",
    "    # 경고문이 뜨기 때문에 \n",
    "    # import urllib3\n",
    "    # urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "    # 을 추가해준다.\n",
    "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "    news = requests.get(url, headers=headers, verify=False)\n",
    "\n",
    "    try:\n",
    "      # BeautifulSoup의 인스턴스 생성한다. 파서는 html.parser를 사용\n",
    "      soup = BeautifulSoup(news.content, 'html.parser')\n",
    "\n",
    "      # CASE 1\n",
    "      news_list = soup.select('.newsflash_body .type06_headline li dl')\n",
    "      # CASE 2\n",
    "      news_list.extend(soup.select('.newsflash_body .type06 li dl'))\n",
    "      page_count += 1\n",
    "      # 마지막 페이지인 경우(없는 페이지를 url에 입력하면 맨 마지막 페이지가 중복으로 나오기 때문에 처음 url을 비교)\n",
    "      if first_url == news_list[0]:\n",
    "        print(\"breakpoint\")\n",
    "        break\n",
    "      # 각 뉴스로부터 a 태그인 <a href ='주소'> 에서 '주소'만을 가져오기\n",
    "      for line in news_list:\n",
    "        url_list.append(line.a.get('href'))\n",
    "      for line in news_list:\n",
    "        news_data.append({\n",
    "          'link': line.a.get('href')\n",
    "        })\n",
    "      first_url = news_list[0]\n",
    "      if page_count >= stop_page :\n",
    "        page_count = 0\n",
    "        await asyncio.sleep(0.5)\n",
    "    except:\n",
    "      print(\"urllist error occured, wait 3 sec\")\n",
    "      await asyncio.sleep(3.0)\n",
    "  print('make_urllist', len(news_data))\n",
    "\n",
    "  # url에 해당하는 뉴스 데이터 받아오기\n",
    "  await make_data(news_code, news_data, doing_date, session, doing_date)\n",
    "\n",
    "\n",
    "# 날짜 단위로 크롤링\n",
    "# 해당 날짜의 url을 다 받으면 시작하며 비동기적으로 진행\n",
    "async def make_data(code, news_data, date, session, doing_date):\n",
    "\n",
    "  save_folder = doing_date[:6]\n",
    "  print(\"readStart!!\" + date)\n",
    "  text_list = []\n",
    "  title_list = []\n",
    "  date_list = []\n",
    "  news_data.reverse() # 뉴스 기사 생성 시간 순으로 정렬\n",
    "  count = 0\n",
    "  for news in news_data:\n",
    "    count += 1\n",
    "    news_url = news['link']\n",
    "\n",
    "    try:\n",
    "      async with session.get(news_url, headers=headers) as res:\n",
    "        html = await res.text()\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        try:\n",
    "          news_title = soup.select_one(\"#title_area\").text.replace(\"\\n\",\"\").replace(\"\\t\",\"\")\n",
    "          news_content = soup.select_one(\"#dic_area\").text.replace(\"\\n\",\"\").replace(\"\\t\",\"\")\n",
    "        except:\n",
    "          continue\n",
    "\n",
    "        try:\n",
    "          html_date = soup.select_one(\"div#ct> div.media_end_head.go_trans > div.media_end_head_info.nv_notrans > div.media_end_head_info_datestamp > div > span\")\n",
    "          news_date = html_date.attrs['data-date-time']\n",
    "        except AttributeError:\n",
    "          news_date = soup.select_one(\"#content > div.end_ct > div > div.article_info > span > em\")\n",
    "\n",
    "        date_list.append(news_date)\n",
    "        title_list.append(news_title)\n",
    "        text_list.append(news_content)\n",
    "\n",
    "      if count >= stop_count:\n",
    "        print(str(doing_date) + \" 기사 \" + str(stop_count) + \"개 입니다.\")\n",
    "        count = 0\n",
    "        await asyncio.sleep(0.5)  # 부하로 인한 차단을 막기 위해 일시정지 후 재시작\n",
    "    except:\n",
    "      print(str(doing_date) + \" 데이터 읽기 오류로 1초 휴식\")\n",
    "      await asyncio.sleep(1.0)\n",
    "      print(str(doing_date) + \"데이터 읽기 재시작\")\n",
    "\n",
    "  df = pd.DataFrame({'date': date_list, 'title': title_list,'news': text_list})\n",
    "  df['code'] = idx2word[str(code)]\n",
    "  date_list[:5]\n",
    "\n",
    "  # 중복된 데이터 제거\n",
    "  df.drop_duplicates(subset=['news'], inplace=True)\n",
    "  print('최종 뉴스 기사의 개수: ',len(df))\n",
    "\n",
    "  # 폴더가 없다면 년월에 해당하는 폴더 생성\n",
    "  try:\n",
    "    if not os.path.exists(save_folder):\n",
    "      os.makedirs(save_folder)\n",
    "  except:\n",
    "    print(date)\n",
    "\n",
    "  # csv 파일로 저장\n",
    "  csv_path = save_folder + \"/\" + str(news_code) + \"-\" + date + \".csv\"\n",
    "  df.to_csv(csv_path, index=False)\n",
    "\n",
    "  if os.path.exists(csv_path):\n",
    "    print('{} File Saved!'.format(csv_path))\n",
    "\n",
    "\n",
    "# 끝 날짜보다 작거나 같을 때 까지만 크롤링 진행\n",
    "async def main():\n",
    "  start_date = '20190607'\n",
    "  end_date = '20190701'\n",
    "  doing_date = start_date\n",
    "  tasks = []\n",
    "  day_count = 0\n",
    "  count_days = 0\n",
    "  async with aiohttp.ClientSession() as session:\n",
    "    while(doing_date <= end_date):\n",
    "      try:\n",
    "        news_data = []\n",
    "        # 해당하는 코드의 뉴스 url 가져오기\n",
    "        task = make_urllist(start, end, news_code, doing_date, news_data, doing_date, session, day_count)\n",
    "        count_days += 1\n",
    "        print(doing_date)\n",
    "\n",
    "        tasks.append(task)\n",
    "\n",
    "        # 10일 단위로 데이터 가져오기\n",
    "        if count_days >= 10 or doing_date == end_date:\n",
    "          await asyncio.gather(*tasks)\n",
    "          tasks = []\n",
    "          count_days = 0\n",
    "          await asyncio.sleep(2.0)  # 부하로 인한 차단을 막기 위해 일시정지 후 재시작\n",
    "\n",
    "      # 예외가 발생했을 경우 (보통 네트워크 에러) 20초 기다린 후 재시작\n",
    "      except RuntimeError:\n",
    "        print(\"런타임에러\")\n",
    "        await asyncio.sleep(20.0)\n",
    "        tasks = []\n",
    "      except:\n",
    "        print(\"error발생, 20초 휴식 후 재시작\")\n",
    "        error_msg = traceback.format_exc()\n",
    "        print(error_msg)\n",
    "        await asyncio.sleep(20.0)\n",
    "        print(\"restart!!!!!\")\n",
    "        # continue\n",
    "\n",
    "\n",
    "      # 날짜 +1 해주기\n",
    "      doing = datetime.datetime.strptime(doing_date, '%Y%m%d')\n",
    "      doing = doing + datetime.timedelta(days = 1)\n",
    "      doing_date = datetime.datetime.strftime(doing, \"%Y%m%d\")\n",
    "\n",
    "    # if len(tasks)>0 :\n",
    "    #   await asyncio.gather(*tasks)\n",
    "    print(\"end\")\n",
    "# .py 파일로 실행시키려면 밑의 줄 실행\n",
    "#asyncio.run(main())\n",
    "await main()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b7b45caa4f4a82c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
