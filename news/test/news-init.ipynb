{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 크롤러를 만들기 전 필요한 도구들을 임포트합니다.\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# 시작 시간과 끝 시간, 현재 하고 있는 시간 지정\n",
    "start_date = '20231228'\n",
    "end_date = '20231231'\n",
    "doing_date = start_date\n",
    "news_code = 101\n",
    "start = 1 # url 크롤링 시작 페이지(default = 1)\n",
    "end = 3 # url 크롤링 끝 페이지\n",
    "\n",
    "# 뉴스 코드 리스트\n",
    "idx2word = {'101' : '경제', '102' : '사회', '103' : '생활/문화', '105' : 'IT/과학'}\n",
    "\n",
    "# 페이지 수, 카테고리, 날짜를 입력값으로 받습니다.\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.90 Safari/537.36'}\n",
    "\n",
    "# url 리스트받아오기\n",
    "def make_urllist(page_start, page_end, code, date): \n",
    "  urllist = []\n",
    "  first_url=''\n",
    "  for i in range(page_start, page_end + 1):\n",
    "    url = 'https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1='+str(code)+'&date='+str(date)+'&page='+str(i)\n",
    "    # headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.90 Safari/537.36'}\n",
    "    news = requests.get(url, headers=headers)\n",
    "\n",
    "    # BeautifulSoup의 인스턴스 생성합니다. 파서는 html.parser를 사용합니다.\n",
    "    soup = BeautifulSoup(news.content, 'html.parser')\n",
    "\n",
    "    # CASE 1\n",
    "    news_list = soup.select('.newsflash_body .type06_headline li dl')\n",
    "    # CASE 2\n",
    "    news_list.extend(soup.select('.newsflash_body .type06 li dl'))\n",
    "    if first_url == news_list[0]:\n",
    "      print(\"breakpoint\")\n",
    "      break\n",
    "    # 각 뉴스로부터 a 태그인 <a href ='주소'> 에서 '주소'만을 가져오기\n",
    "    for line in news_list:\n",
    "      urllist.append(line.a.get('href'))\n",
    "    for line in news_list:\n",
    "      newsData.append({\n",
    "      'link': line.a.get('href')\n",
    "      })\n",
    "    first_url = news_list[0]\n",
    "  print('make_urllist', len(newsData))\n",
    "  return urllist\n",
    "\n",
    "# 시간단위로 크롤링해보기\n",
    "def make_data(urllist, code):\n",
    "  text_list = []\n",
    "  date_list = []\n",
    "  newsData.reverse()\n",
    "  for news in newsData:\n",
    "    news_url = news['link']\n",
    "    res = requests.get(news_url, headers=headers)\n",
    "    soup = BeautifulSoup(res.text, 'lxml')\n",
    "    try:\n",
    "      news_content = soup.select_one(\"#dic_area\").text.replace(\"\\n\",\"\").replace(\"\\t\",\"\")\n",
    "    except:\n",
    "      continue\n",
    "    \n",
    "    try:\n",
    "      html_date = soup.select_one(\"div#ct> div.media_end_head.go_trans > div.media_end_head_info.nv_notrans > div.media_end_head_info_datestamp > div > span\")\n",
    "      news_date = html_date.attrs['data-date-time']\n",
    "    except AttributeError:\n",
    "      news_date = soup.select_one(\"#content > div.end_ct > div > div.article_info > span > em\")\n",
    "    \n",
    "    date_list.append(news_date)\n",
    "    text_list.append(news_content)\n",
    "    \n",
    "  df = pd.DataFrame({'date': date_list, 'news': text_list})\n",
    "  df['code'] = idx2word[str(code)]\n",
    "  date_list[:5]\n",
    "  print(len(df))\n",
    "  # 중복된 데이터 제거\n",
    "  df.drop_duplicates(subset=['news'], inplace=True)\n",
    "  print('뉴스 기사의 개수: ',len(df))\n",
    "  return df\n",
    "\n",
    "\n",
    "# 끝 시간보다 작거나 같을 때 까지만 크롤링 진행\n",
    "while(doing_date <= end_date):\n",
    "  newsData = []\n",
    "  # 경제 파트의 url 가져오기\n",
    "  url_list = make_urllist(start, end, news_code, doing_date)\n",
    "  # 가져온 파트의 뉴스 기사 가져오기\n",
    "  data = make_data(url_list, news_code)\n",
    "  \n",
    "  print(doing_date)\n",
    "  print('뉴스 기사의 개수: ',len(url_list))\n",
    "  print('뉴스 데이터 개수: ',len(newsData))\n",
    "  print(len(data))\n",
    "  \n",
    "  # 데이터프레임 파일을 csv 파일로 저장\n",
    "  csv_path = str(news_code) + \"-\" + doing_date + \".csv\"\n",
    "  data.to_csv(csv_path, index=False)\n",
    "\n",
    "  if os.path.exists(csv_path):\n",
    "    print('{} File Saved!'.format(csv_path))\n",
    "    \n",
    "  # 날짜 +1 해주기\n",
    "  doing = datetime.datetime.strptime(doing_date, '%Y%m%d')\n",
    "  doing = doing + datetime.timedelta(days = 1)\n",
    "  doing_date = datetime.datetime.strftime(doing, \"%Y%m%d\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b7b45caa4f4a82c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
