{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 클러스터링 번호 1번이 식품 관련이기 때문에\n",
    "# 1만 클러스터링\n",
    "\n",
    "# 2차 클러스터링 코드\n",
    "# HuggingFace 사용을 위해 터미널에서 추가하기 위한 것\n",
    "# pip install -U sentence-transformers\n",
    "\n",
    "# Sentence-Transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"안녕하세요?\", \"한국어 문장 임베딩을 위한 버트 모델입니다.\"]\n",
    "\n",
    "model = SentenceTransformer('jhgan/ko-sroberta-multitask')\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# HuggingFace Transformers\n",
    "# 클러스터링을 하기 전 임베딩을 위한...\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# 데이터 클러스터링을 위한 사이킷런 설치\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# pip install scikit-learn\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "import json # json 파일을 읽고 쓰기 위한\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle   # 모델을 저장하기 위한\n",
    "\n",
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# 1차 클러스터링 된 데이터 가져오기\n",
    "year = 2022\n",
    "newscount_end = 25\n",
    "df = pd.read_csv('Clustered/' + str(year) + '-news-clustered-fullver1.csv')\n",
    "\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('jhgan/ko-sroberta-multitask')\n",
    "model = AutoModel.from_pretrained('jhgan/ko-sroberta-multitask')\n",
    "\n",
    "second_vector = [] # 차원 정보를 저장\n",
    "\n",
    "# 필터링 된 데이터 가져오기\n",
    "# filtered_data = \"data_filtered/ssafy_dataset_news_\" + str(year) + \".csv-\" + str(i) + \".json\"\n",
    "#filtered_data = \"2\" + str(year) + \".csv-\" + str(i) + \".json\"\n",
    "count = 0\n",
    "# 필터링 된 데이터를 파일로 가져오기\n",
    "# Sentences we want sentence embeddings for\n",
    "#sentences = ['This is an example sentence', 'Each sentence is converted']  \n",
    "# with open(filtered_data) as file:\n",
    "#     filtered = json.load(file)  # json 파일 읽어오기(배열로 1개씩 가져올 수 있음)\n",
    "\n",
    "clustered_news = df.loc[df['cluster_number'] == 1, :]\n",
    "print(len(clustered_news))\n",
    "clust_news = clustered_news['filtered_news']\n",
    "clust_num = clustered_news['cluster_number']\n",
    "\n",
    "\n",
    "# 1차 클러스터링 된 정보를 바탕으로 2차 클러스터링\n",
    "for num, article in zip(clust_num, clust_news):\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(article, padding = True, truncation = True, return_tensors='pt')\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling. In this case, mean pooling.\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    # 차원축소를 위한 코드(사이킷런은 2차원까지만 알아먹기 때문에)\n",
    "    dimension_down = np.squeeze(sentence_embeddings, axis = 0)\n",
    "\n",
    "    # 여기까지가 임베딩을 위한 코드\n",
    "\n",
    "    count+= 1\n",
    "    second_vector.append(dimension_down)\n",
    "\n",
    "    if count>=300:\n",
    "        print(\"현재 데이터 \"+str(count)+\"개 실행 완료\")\n",
    "        count = 0\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"!! 모든 데이터 필터링 완료 !!\")\n",
    "\n",
    "\n",
    "# 클러스터링을 위한 모델, DBSCAN을 사용, cosine 계산 수식 사용\n",
    "second_filtered_model = DBSCAN(eps = 0.15, min_samples = 4, metric = \"cosine\")\n",
    "# eps 0.15인게 그나마 나음\n",
    "# 아니면 0.185, 샘플3\n",
    "# 0.18, 샘플 4\n",
    "# 0.127, 3\n",
    "# 학습된것을 fit을 사용해 모델로 따로 저장(저장 안하려면 fit_predict 사용하면 됨)\n",
    "second_result = second_filtered_model.fit_predict(second_vector)\n",
    "# save_second = second_filtered_model.fit(second_vector)\n",
    "\n",
    "\n",
    "# with open('saved_model_2nd_fullver1', 'wb') as f:\n",
    "#     pickle.dump(save_second, f)\n",
    "# print(\"학습 모델 저장\")\n",
    "\n",
    "# # # 모델을 불러 올 때\n",
    "# # with open('saved_model', 'rb') as f:\n",
    "# #     mod = pickle.load(f)\n",
    "# # # 이후 mod 로 모델 사용하면 됨\n",
    "\n",
    "final_filter = []\n",
    "cluster_num = []\n",
    "for vec, clust, fil in zip(second_vector, second_result, clust_news):\n",
    "    if clust == -1:\n",
    "        continue\n",
    "    else:\n",
    "        # print(\"cluster num: \" + str(clust))\n",
    "        # print(fil)\n",
    "        final_filter.append(fil)\n",
    "        cluster_num.append(clust)\n",
    "\n",
    "df = pd.DataFrame({'cluster_number': cluster_num, 'filtered_news': final_filter})\n",
    "df.sort_values(by=['cluster_number'], inplace=True)\n",
    "csv_path = \"Clustered/2022-2nd-full-clustered-ver2.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(\"데이터 클러스터링 완료\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
